{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TNtC7c48x1vE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
        "Information Gain is a concept from information theory that is used in decision tree algorithms to decide the best feature for splitting the data at each node. It measures how much uncertainty is reduced after splitting the dataset based on a particular attribute. In simple words, Information Gain tells us which feature gives the most useful information for making a decision.\n",
        "\n",
        "Information Gain is based on a metric called Entropy, which measures the impurity or randomness in a dataset. If a dataset contains mixed classes, entropy is high. If all data points belong to a single class, entropy is zero.\n",
        "\n",
        "The formula for entropy is:\n",
        "\n",
        "Entropy(S) = − Σ pᵢ log₂(pᵢ)\n",
        "\n",
        "where pᵢ is the probability of class i in dataset S.\n",
        "\n",
        "Information Gain is calculated using the following formula:\n",
        "\n",
        "Information Gain(S, A) = Entropy(S) − Σ (|Sᵥ| / |S|) Entropy(Sᵥ)\n",
        "\n",
        "where:\n",
        "\n",
        "S is the original dataset\n",
        "\n",
        "A is the attribute used for splitting\n",
        "\n",
        "Sᵥ is the subset created after the split\n",
        "\n",
        "Use of Information Gain in Decision Trees\n",
        "\n",
        "In decision tree algorithms like ID3, Information Gain is used to select the best attribute for splitting the data at each node. The attribute with the highest Information Gain is chosen because it results in the most homogeneous child nodes.\n",
        "\n",
        "For example, in a decision tree used to predict whether a customer will buy a house, features like income, location, and age may be considered. Information Gain helps determine which feature best separates buyers from non-buyers at each step.\n",
        "\n",
        "Advantages of Information Gain\n",
        "\n",
        "Helps in building efficient and accurate decision trees\n",
        "\n",
        "Reduces uncertainty at each split\n",
        "\n",
        "Easy to understand and implement\n",
        "\n",
        "Limitations of Information Gain\n",
        "\n",
        "It tends to favor attributes with many distinct values\n",
        "\n",
        "May lead to overfitting in some cases\n",
        "\n",
        "To overcome this limitation, algorithms like C4.5 use Gain Ratio instead of Information Gain.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "In conclusion, Information Gain is an important criterion used in decision trees to select the best attribute for data splitting. By reducing entropy at each step, it helps create clear and meaningful decision boundaries, leading to better classification performance.\n",
        "\n",
        "\n",
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "Gini Impurity and Entropy are two important impurity measures used in decision tree algorithms to evaluate the quality of a split. Both are used to measure how mixed or impure the classes are in a dataset, but they differ in calculation, interpretation, and usage. Gini Impurity measures the probability that a randomly selected data point would be incorrectly classified if it were randomly labeled according to the class distribution in a node. It is calculated using the formula Gini = 1 − Σp², where p represents the probability of each class. Entropy, on the other hand, measures the amount of uncertainty or randomness present in the data and is calculated using the formula Entropy = −Σp log₂(p).\n",
        "\n",
        "The range of Gini Impurity for binary classification lies between 0 and 0.5, whereas the range of Entropy lies between 0 and 1. In both cases, a value of zero indicates a completely pure node. From a computational point of view, Gini Impurity is faster to calculate because it does not involve logarithmic operations, while Entropy is computationally more expensive due to the use of logarithms. Entropy is more sensitive to changes in class distribution and generally produces more balanced decision trees, whereas Gini Impurity is slightly less sensitive and may favor larger partitions.\n",
        "\n",
        "In terms of usage, Entropy is mainly used in algorithms such as ID3 and C4.5, while Gini Impurity is used in the CART algorithm. Gini Impurity is preferred when working with large datasets where computational efficiency is important, while Entropy is preferred when interpretability and balanced splits are required. In conclusion, both Gini Impurity and Entropy serve the same purpose of measuring impurity in decision trees, but the choice between them depends on dataset size, computational efficiency, and the decision tree algorithm being used.\n",
        "\n",
        "Question 3:What is Pre-Pruning in Decision Trees?\n",
        "Pre-pruning in decision trees is a technique used to stop the growth of the tree at an early stage in order to prevent overfitting and improve the generalization performance of the model. In decision tree learning, if a tree is allowed to grow fully, it may become very complex and start memorizing the training data instead of learning the actual patterns. Pre-pruning controls this problem by applying stopping criteria before the tree becomes too deep.\n",
        "\n",
        "In pre-pruning, the algorithm checks certain conditions while building the tree and stops further splitting if those conditions are not satisfied. Common pre-pruning criteria include setting a maximum depth of the tree, specifying a minimum number of samples required to split a node, limiting the minimum number of samples in a leaf node, or defining a threshold for minimum information gain or Gini reduction. If a split does not provide sufficient improvement, the algorithm does not create further branches.\n",
        "\n",
        "The main advantage of pre-pruning is that it reduces overfitting and improves the model’s performance on unseen data. It also makes the decision tree simpler, faster to train, and easier to interpret. However, pre-pruning has a limitation because it may stop the tree too early and miss important patterns in the data, leading to underfitting.\n",
        "\n",
        "In practical applications, pre-pruning is widely used when working with large datasets or noisy data, where uncontrolled tree growth can reduce prediction accuracy. By carefully selecting pre-pruning parameters, a balance can be achieved between model simplicity and prediction accuracy. In conclusion, pre-pruning is an effective method to control tree complexity, reduce overfitting, and build a robust and efficient decision tree model.\n",
        "\n",
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "A Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for both classification and regression problems. The main idea behind SVM is to find an optimal decision boundary, called a hyperplane, that separates data points of different classes with the maximum possible margin. The margin is defined as the distance between the hyperplane and the nearest data points from each class, which are known as support vectors.\n",
        "\n",
        "SVM works by transforming the input data into a higher-dimensional space where a clear separation between classes becomes possible. In cases where the data is linearly separable, SVM finds a straight line (in two dimensions) or a flat hyperplane (in higher dimensions) to classify the data. However, for non-linearly separable data, SVM uses a technique called the kernel trick, which allows the algorithm to create complex, non-linear decision boundaries without explicitly computing the higher-dimensional transformation.\n",
        "\n",
        "Commonly used kernels include linear, polynomial, radial basis function (RBF), and sigmoid kernels. The choice of kernel plays a crucial role in the performance of the SVM model. SVM also uses a regularization parameter, usually denoted as C, which controls the trade-off between maximizing the margin and minimizing classification errors.\n",
        "\n",
        "SVM is widely used in applications such as image recognition, text classification, bioinformatics, and face detection because of its high accuracy and ability to handle high-dimensional data. However, SVM can be computationally expensive for very large datasets and requires careful tuning of parameters.\n",
        "\n",
        "In conclusion, Support Vector Machine is a robust and effective machine learning algorithm that focuses on maximizing the margin between classes to achieve better generalization and high predictive performance.\n",
        "\n",
        "\n",
        "Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "\n",
        "The kernel trick in Support Vector Machines (SVM) is a powerful technique that allows the algorithm to handle non-linearly separable data by implicitly mapping the input features into a higher-dimensional feature space. In many real-world problems, data cannot be separated using a straight line or a simple hyperplane. The kernel trick helps SVM overcome this limitation without explicitly computing the transformation, which makes the process computationally efficient.\n",
        "\n",
        "In simple terms, the kernel trick works by replacing the dot product of input vectors with a kernel function. This kernel function computes the similarity between data points in a higher-dimensional space while performing calculations in the original space. As a result, SVM can construct complex, non-linear decision boundaries while maintaining efficiency.\n",
        "\n",
        "Commonly used kernel functions include the Linear Kernel, which is used when data is linearly separable; the Polynomial Kernel, which captures polynomial relationships; the Radial Basis Function (RBF) Kernel, which is effective for complex and highly non-linear data; and the Sigmoid Kernel, which resembles the behavior of neural networks. The choice of kernel depends on the nature of the dataset.\n",
        "\n",
        "The kernel trick is important because it avoids the computational cost of explicitly transforming data into high-dimensional space, which could be extremely expensive or even impossible. It allows SVM to work efficiently even when the feature space is very large or infinite.\n",
        "\n",
        "In conclusion, the kernel trick is a key concept that makes SVM a versatile and powerful algorithm. By enabling non-linear classification through efficient computations, it allows SVM to solve complex real-world problems with high accuracy.\n",
        "\n",
        "Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "The Naïve Bayes classifier is a supervised machine learning algorithm based on Bayes’ Theorem, which is used mainly for classification tasks. It predicts the class of a given data point by calculating the probability of each class given the input features and selecting the class with the highest probability. Naïve Bayes is widely used in applications such as spam detection, sentiment analysis, document classification, and medical diagnosis due to its simplicity and efficiency.\n",
        "\n",
        "The working of the Naïve Bayes classifier is based on Bayes’ Theorem, which is expressed as\n",
        "P(C|X) = [P(X|C) × P(C)] / P(X),\n",
        "where P(C|X) is the posterior probability of class C given features X, P(X|C) is the likelihood, P(C) is the prior probability of the class, and P(X) is the evidence.\n",
        "\n",
        "The classifier is called “Naïve” because it makes a strong and unrealistic assumption that all input features are conditionally independent of each other given the class label. This means that the presence or absence of one feature does not affect the presence or absence of another feature, which is rarely true in real-world data. Despite this naive assumption, the classifier often performs surprisingly well, especially on large datasets.\n",
        "\n",
        "One of the main advantages of Naïve Bayes is that it is computationally efficient, requires a small amount of training data, and works well with high-dimensional data. However, its main limitation is that the independence assumption may reduce accuracy when features are highly correlated.\n",
        "\n",
        "In conclusion, the Naïve Bayes classifier is a simple yet powerful probabilistic classification algorithm. It is called “naïve” because of its assumption of feature independence, but even with this simplification, it remains an effective and widely used classification technique in machine learning.\n",
        "\n",
        "\n",
        "\n",
        "Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Baye\n",
        "\n",
        "Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes are three important variants of the Naïve Bayes classifier, and they mainly differ in the type of data they are designed to handle and how they model feature distributions. All three are based on Bayes’ Theorem and follow the same assumption that features are conditionally independent given the class label, but they are applied in different situations.\n",
        "\n",
        "Gaussian Naïve Bayes is used when the input features are continuous numerical values and are assumed to follow a normal (Gaussian) distribution. It calculates probabilities using the mean and variance of each feature for every class. This type is commonly used in problems such as medical diagnosis, sensor data analysis, and real-valued measurements where data is continuous.\n",
        "\n",
        "Multinomial Naïve Bayes is mainly used for discrete count-based data, especially in text classification tasks. It works well with features such as word frequencies or term counts. Multinomial Naïve Bayes assumes that the features represent the number of times a word appears in a document. It is widely used in spam detection, document categorization, and sentiment analysis.\n",
        "\n",
        "Bernoulli Naïve Bayes is used when the features are binary, meaning they take values such as 0 or 1, indicating the presence or absence of a feature. Instead of counting how many times a word appears, Bernoulli Naïve Bayes only considers whether the word is present or not. This makes it suitable for applications where binary feature representation is important.\n",
        "\n",
        "In summary, Gaussian Naïve Bayes is best suited for continuous data, Multinomial Naïve Bayes is ideal for text and count-based data, and Bernoulli Naïve Bayes is used for binary features. Choosing the correct variant depends on the nature of the dataset. Using the appropriate Naïve Bayes model improves classification accuracy and overall performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "smKDoolRx173"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Question 4:Write a Python program to train a Decision Tree Classifier using Gini\n",
        "#Impurity as the criterion and print the feature importances (practical).\n",
        "#Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# Load sample dataset (Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data      # Features\n",
        "y = data.target    # Target labels\n",
        "\n",
        "# Create Decision Tree Classifier using Gini Impurity\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = model.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better display\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': data.feature_names,\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# Print feature importances\n",
        "print(importance_df)\n"
      ],
      "metadata": {
        "id": "TUogZMht7XAb",
        "outputId": "71fda828-11d9-4eb3-ddce-6ce2c40c46e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Feature  Importance\n",
            "0  sepal length (cm)    0.013333\n",
            "1   sepal width (cm)    0.000000\n",
            "2  petal length (cm)    0.564056\n",
            "3   petal width (cm)    0.422611\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 10: Breast Cancer Dataset\n",
        "#Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "#dataset and evaluate accuracy.\n",
        "\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data      # Features\n",
        "y = data.target    # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create Gaussian Naïve Bayes classifier\n",
        "model = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Accuracy of Gaussian Naïve Bayes Classifier:\", accuracy)\n"
      ],
      "metadata": {
        "id": "RtqIbv6C8Ga7",
        "outputId": "9029f73b-7f90-4c65-8ba3-987c784dc756",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naïve Bayes Classifier: 0.9736842105263158\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}